"""Core types for the AI Evolution Platform."""

from dataclasses import dataclass, field
from datetime import datetime
from typing import Any


@dataclass
class GenerateResult:
    """
    Result of an adapter generate() call when trace linkage is needed.

    Adapters may return GenerateResult instead of raw output so that
    trace_id and observation_id can be attached to scores (e.g. for Langfuse).
    """

    output: Any
    trace_id: str | None = None
    observation_id: str | None = None
    metadata: dict[str, Any] = field(default_factory=dict)


def normalize_adapter_output(result: Any) -> tuple[Any, str | None, str | None]:
    """
    Normalize adapter generate() return value to (output, trace_id, observation_id).

    If result is GenerateResult, returns (result.output, result.trace_id, result.observation_id).
    Otherwise returns (result, None, None). Use this so scorers always get the raw output
    and callers can attach trace_id/observation_id to Score when present.
    """
    if isinstance(result, GenerateResult):
        return (result.output, result.trace_id, result.observation_id)
    return (result, None, None)


@dataclass
class Score:
    """Evaluation score with metadata.

    metadata may include cost-related keys when linked to a trace (via trace_id):
    cost, input_tokens, output_tokens, provider, model (see docs/tracing.md).
    """

    name: str
    value: float | bool
    eval_id: str
    comment: str | None = None
    metadata: dict[str, Any] = field(default_factory=dict)
    trace_id: str | None = None  # For Langfuse linking
    observation_id: str | None = None  # For Langfuse linking

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary."""
        return {
            "name": self.name,
            "value": self.value,
            "eval_id": self.eval_id,
            "comment": self.comment,
            "metadata": self.metadata,
            "trace_id": self.trace_id,
            "observation_id": self.observation_id,
        }


@dataclass
class EvalResult:
    """Result of executing an eval.

    Represents the complete result of running an evaluation against a dataset.
    Contains all scores generated by scorers for each dataset item.

    metadata may include aggregate_metrics: { accuracy, cost, latency_sec,
    input_tokens, output_tokens } for run-level summary (see docs/tracing.md).
    """

    eval_id: str
    run_id: str
    dataset_id: str
    scores: list[Score]
    metadata: dict[str, Any] = field(default_factory=dict)
    created_at: datetime = field(default_factory=datetime.now)

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary."""
        return {
            "eval_id": self.eval_id,
            "run_id": self.run_id,
            "dataset_id": self.dataset_id,
            "scores": [score.to_dict() for score in self.scores],
            "metadata": self.metadata,
            "created_at": self.created_at.isoformat(),
        }


@dataclass
class DatasetItem:
    """Single item in a dataset."""

    id: str
    input: dict[str, Any]
    output: Any | None = None
    expected: dict[str, Any] | None = None
    tags: list[str] = field(default_factory=list)
    metadata: dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary."""
        result = {
            "id": self.id,
            "input": self.input,
            "tags": self.tags,
            "metadata": self.metadata,
        }
        if self.output is not None:
            result["output"] = self.output
        if self.expected is not None:
            result["expected"] = self.expected
        return result
